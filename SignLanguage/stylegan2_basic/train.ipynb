{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14898,
     "status": "ok",
     "timestamp": 1610301893082,
     "user": {
      "displayName": "이도연",
      "photoUrl": "",
      "userId": "01121628625520708341"
     },
     "user_tz": -540
    },
    "id": "Pnl9bdM-1K4-",
    "outputId": "3c199001-d274-4504-d447-5ba9ce0608ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2409,
     "status": "ok",
     "timestamp": 1610301893082,
     "user": {
      "displayName": "이도연",
      "photoUrl": "",
      "userId": "01121628625520708341"
     },
     "user_tz": -540
    },
    "id": "xPvQeeEH1S8N"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/gdrive/MyDrive/stylegan2_tobigs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1994,
     "status": "ok",
     "timestamp": 1610304483122,
     "user": {
      "displayName": "이도연",
      "photoUrl": "",
      "userId": "01121628625520708341"
     },
     "user_tz": -540
    },
    "id": "h2AHAGH31cn0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import str_to_bool\n",
    "from tf_utils import check_tf_version, allow_memory_growth, split_gpu_for_testing\n",
    "from load_models import load_generator, load_discriminator\n",
    "from dataset.get_tfrecords import get_dataset\n",
    "from losses import d_logistic, d_logistic_r1_reg, g_logistic_non_saturating, g_logistic_ns_pathreg\n",
    "from model.utils import merge_batch_images\n",
    "\n",
    "\n",
    "def initiate_models(g_params, d_params):\n",
    "    discriminator = load_discriminator(d_params, ckpt_dir=None)\n",
    "    generator = load_generator(g_params=g_params, is_g_clone=False, ckpt_dir=None)\n",
    "    g_clone = load_generator(g_params=g_params, is_g_clone=True, ckpt_dir=None)\n",
    "\n",
    "    # set initial g_clone weights same as generator\n",
    "    g_clone.set_weights(generator.get_weights())\n",
    "    return discriminator, generator, g_clone\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, t_params, name):\n",
    "        self.cur_tf_ver = t_params['cur_tf_ver']\n",
    "        self.use_tf_function = t_params['use_tf_function']\n",
    "        self.use_custom_cuda = t_params['use_custom_cuda']\n",
    "        self.model_base_dir = t_params['model_base_dir']\n",
    "        self.global_batch_size = t_params['batch_size']\n",
    "        self.n_total_image = t_params['n_total_image']\n",
    "        self.max_steps = int(np.ceil(self.n_total_image / self.global_batch_size))\n",
    "        self.n_samples = min(t_params['batch_size'], t_params['n_samples'])\n",
    "        self.train_res = t_params['train_res']\n",
    "        self.print_step = 100\n",
    "        self.save_step = 100\n",
    "        self.image_summary_step = 100\n",
    "        self.reached_max_steps = False\n",
    "\n",
    "        # copy network params\n",
    "        self.g_params = t_params['g_params']\n",
    "        self.d_params = t_params['d_params']\n",
    "\n",
    "        # set optimizer params\n",
    "        self.global_batch_scaler = 1.0 / float(self.global_batch_size)\n",
    "        self.r1_gamma = 10.0\n",
    "        self.g_opt = self.update_optimizer_params(t_params['g_opt'])\n",
    "        self.d_opt = self.update_optimizer_params(t_params['d_opt'])\n",
    "        self.pixel_lambda = self.g_opt['pixel_lambda']\n",
    "        self.idt_lambda = self.d_opt['idt_lambda']\n",
    "        self.pl_minibatch_shrink = 2\n",
    "        self.pl_weight = float(self.pl_minibatch_shrink)\n",
    "        self.pl_denorm = tf.math.rsqrt(float(self.train_res) * float(self.train_res))\n",
    "        self.pl_decay = 0.01\n",
    "        self.pl_mean = tf.Variable(initial_value=0.0, name='pl_mean', trainable=False,\n",
    "                                   synchronization=tf.VariableSynchronization.ON_READ,\n",
    "                                   aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\n",
    "\n",
    "        # create model: model and optimizer must be created under `strategy.scope`\n",
    "        self.discriminator, self.generator, self.g_clone = initiate_models(self.g_params,\n",
    "                                                                           self.d_params)\n",
    "\n",
    "        # set optimizers\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(self.d_opt['learning_rate'],\n",
    "                                                    beta_1=self.d_opt['beta1'],\n",
    "                                                    beta_2=self.d_opt['beta2'],\n",
    "                                                    epsilon=self.d_opt['epsilon'])\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(self.g_opt['learning_rate'],\n",
    "                                                    beta_1=self.g_opt['beta1'],\n",
    "                                                    beta_2=self.g_opt['beta2'],\n",
    "                                                    epsilon=self.g_opt['epsilon'])\n",
    "\n",
    "        # setup saving locations (object based savings)\n",
    "        self.ckpt_dir = os.path.join(self.model_base_dir, name)\n",
    "        self.ckpt = tf.train.Checkpoint(d_optimizer=self.d_optimizer,\n",
    "                                        g_optimizer=self.g_optimizer,\n",
    "                                        discriminator=self.discriminator,\n",
    "                                        generator=self.generator,\n",
    "                                        g_clone=self.g_clone,\n",
    "                                        pl_mean=self.pl_mean)\n",
    "        self.manager = tf.train.CheckpointManager(self.ckpt, self.ckpt_dir, max_to_keep=2)\n",
    "\n",
    "        # try to restore\n",
    "        self.ckpt.restore(self.manager.latest_checkpoint)\n",
    "        if self.manager.latest_checkpoint:\n",
    "            print('Restored from {}'.format(self.manager.latest_checkpoint))\n",
    "\n",
    "            # check if already trained in this resolution\n",
    "            restored_step = self.g_optimizer.iterations.numpy()\n",
    "            if restored_step >= self.max_steps:\n",
    "                print('Already reached max steps {}/{}'.format(restored_step, self.max_steps))\n",
    "                self.reached_max_steps = True\n",
    "                return\n",
    "        else:\n",
    "            print('Not restoring from saved checkpoint')\n",
    "\n",
    "    @staticmethod\n",
    "    def update_optimizer_params(params):\n",
    "        params_copy = params.copy()\n",
    "        mb_ratio = params_copy['reg_interval'] / (params_copy['reg_interval'] + 1)\n",
    "        params_copy['learning_rate'] = params_copy['learning_rate'] * mb_ratio\n",
    "        params_copy['beta1'] = params_copy['beta1'] ** mb_ratio\n",
    "        params_copy['beta2'] = params_copy['beta2'] ** mb_ratio\n",
    "        return params_copy\n",
    "\n",
    "    @tf.function\n",
    "    def d_train_step(self, images, labels):\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            # compute losses\n",
    "            d_gan_loss, idt_loss = d_logistic(images, labels, self.generator, self.discriminator)\n",
    "\n",
    "            # scale loss\n",
    "            d_gan_loss = tf.reduce_sum(d_gan_loss) * self.global_batch_scaler\n",
    "            idt_loss = tf.reduce_sum(idt_loss) * self.global_batch_scaler\n",
    "\n",
    "            d_loss = d_gan_loss + idt_loss * self.idt_lambda\n",
    "\n",
    "        d_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n",
    "        return d_loss, d_gan_loss, idt_loss\n",
    "\n",
    "    @tf.function\n",
    "    def d_train_step_reg(self, images, labels):\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            # compute losses\n",
    "            d_gan_loss, idt_loss, r1_penalty = d_logistic_r1_reg(images, labels, self.generator, self.discriminator)\n",
    "            r1_penalty = r1_penalty * (0.5 * self.r1_gamma) * self.d_opt['reg_interval']\n",
    "\n",
    "            # scale losses\n",
    "            r1_penalty = tf.reduce_sum(r1_penalty) * self.global_batch_scaler\n",
    "            d_gan_loss = tf.reduce_sum(d_gan_loss) * self.global_batch_scaler\n",
    "            idt_loss = tf.reduce_sum(idt_loss) * self.global_batch_scaler\n",
    "\n",
    "            # combine\n",
    "            d_loss = d_gan_loss + idt_loss * self.idt_lambda + r1_penalty\n",
    "\n",
    "        d_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n",
    "        return d_loss, d_gan_loss, idt_loss, r1_penalty\n",
    "\n",
    "    @tf.function\n",
    "    def g_train_step(self, images, labels):\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            # compute losses\n",
    "            g_gan_loss, pixel_loss = g_logistic_non_saturating(images, labels, self.generator, self.discriminator)\n",
    "\n",
    "            # scale loss\n",
    "            g_gan_loss = tf.reduce_sum(g_gan_loss) * self.global_batch_scaler\n",
    "            pixel_loss = tf.reduce_sum(pixel_loss) * self.global_batch_scaler\n",
    "\n",
    "            g_loss = g_gan_loss + pixel_loss * self.pixel_lambda\n",
    "\n",
    "        g_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
    "        return g_loss, g_gan_loss, pixel_loss\n",
    "\n",
    "    @tf.function\n",
    "    def g_train_step_reg(self, images, labels):\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            # compute losses\n",
    "            g_gan_loss, pixel_loss, pl_penalty = g_logistic_ns_pathreg(images, labels, self.generator, self.discriminator,\n",
    "                                                           self.pl_mean, self.pl_minibatch_shrink, self.pl_denorm, self.pl_decay)\n",
    "            pl_penalty = pl_penalty * self.pl_weight * self.g_opt['reg_interval']\n",
    "\n",
    "            # scale loss\n",
    "            pl_penalty = tf.reduce_sum(pl_penalty) * self.global_batch_scaler\n",
    "            g_gan_loss = tf.reduce_sum(g_gan_loss) * self.global_batch_scaler\n",
    "            pixel_loss = tf.reduce_sum(pixel_loss) * self.global_batch_scaler\n",
    "\n",
    "            # combine\n",
    "            g_loss = g_gan_loss + pixel_loss * self.pixel_lambda + pl_penalty\n",
    "\n",
    "        g_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
    "        return g_loss, g_gan_loss, pixel_loss, pl_penalty\n",
    "\n",
    "    def train(self, train_ds, val_ds):\n",
    "        if self.reached_max_steps:\n",
    "            return\n",
    "\n",
    "        # create iterator for validation dataset\n",
    "        val_ds_iter = iter(val_ds)\n",
    "\n",
    "        # start actual training\n",
    "\n",
    "        # setup tensorboards\n",
    "        train_summary_writer = tf.summary.create_file_writer(self.ckpt_dir)\n",
    "\n",
    "        # loss metrics\n",
    "        metric_d_loss = tf.keras.metrics.Mean('d_loss', dtype=tf.float32)\n",
    "        metric_g_loss = tf.keras.metrics.Mean('g_loss', dtype=tf.float32)\n",
    "        metric_d_gan_loss = tf.keras.metrics.Mean('d_gan_loss', dtype=tf.float32)\n",
    "        metric_g_gan_loss = tf.keras.metrics.Mean('g_gan_loss', dtype=tf.float32)\n",
    "        metric_idt_loss = tf.keras.metrics.Mean('metric_idt_loss', dtype=tf.float32)\n",
    "        metric_pixel_loss = tf.keras.metrics.Mean('metric_pixel_loss', dtype=tf.float32)\n",
    "        metric_r1_penalty = tf.keras.metrics.Mean('r1_penalty', dtype=tf.float32)\n",
    "        metric_pl_penalty = tf.keras.metrics.Mean('pl_penalty', dtype=tf.float32)\n",
    "\n",
    "        \n",
    "        # start training\n",
    "        print('Start Training')\n",
    "        print('max_steps: {}'.format(self.max_steps))\n",
    "        t_start = time.time()\n",
    "        \n",
    "        zero = tf.constant(0.0, dtype=tf.float32)\n",
    "        for real_images, labels in train_ds:\n",
    "            step = self.g_optimizer.iterations.numpy()\n",
    "\n",
    "            # d train step\n",
    "            if (step + 1) % self.d_opt['reg_interval'] == 0:\n",
    "                d_loss, d_gan_loss, idt_loss, r1_penalty = self.d_train_step_reg(real_images, labels)\n",
    "            else:\n",
    "                d_loss, d_gan_loss, idt_loss = self.d_train_step(real_images, labels)\n",
    "                r1_penalty = zero\n",
    "\n",
    "            # g train step\n",
    "            if (step + 1) % self.g_opt['reg_interval'] == 0:\n",
    "                g_loss, g_gan_loss, pixel_loss, pl_penalty = self.g_train_step_reg(real_images, labels)\n",
    "            else:\n",
    "                g_loss, g_gan_loss, pixel_loss = self.g_train_step(real_images, labels)\n",
    "                pl_penalty = zero\n",
    "\n",
    "            # update g_clone\n",
    "            self.g_clone.set_as_moving_average_of(self.generator)\n",
    "\n",
    "            # update metrics\n",
    "            metric_d_loss(d_loss)\n",
    "            metric_g_loss(g_loss)\n",
    "            metric_d_gan_loss(d_gan_loss)\n",
    "            metric_g_gan_loss(g_gan_loss)\n",
    "            metric_idt_loss(idt_loss)\n",
    "            metric_pixel_loss(pixel_loss)\n",
    "            metric_r1_penalty(r1_penalty)\n",
    "            metric_pl_penalty(pl_penalty)\n",
    "\n",
    "            # get current step\n",
    "            step = self.g_optimizer.iterations.numpy()\n",
    "\n",
    "            # save to tensorboard\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('d_loss', metric_d_loss.result(), step=step)\n",
    "                tf.summary.scalar('g_loss', metric_g_loss.result(), step=step)\n",
    "                tf.summary.scalar('d_gan_loss', metric_d_gan_loss.result(), step=step)\n",
    "                tf.summary.scalar('g_gan_loss', metric_g_gan_loss.result(), step=step)\n",
    "                tf.summary.scalar('idt_loss', metric_idt_loss.result(), step=step)\n",
    "                tf.summary.scalar('pixel_loss', metric_pixel_loss.result(), step=step)\n",
    "                tf.summary.scalar('r1_penalty', metric_r1_penalty.result(), step=step)\n",
    "                tf.summary.scalar('pl_penalty', metric_pl_penalty.result(), step=step)\n",
    "\n",
    "            # print every self.print_steps\n",
    "            if step % self.print_step == 0:\n",
    "                elapsed = time.time() - t_start\n",
    "                print(f'[step {step}: elapsed: {elapsed:.2f}s] '\n",
    "                      f'd_total: {metric_d_loss.result():.3f}, '\n",
    "                      f'd_gan_loss: {metric_d_gan_loss.result():.3f}, '\n",
    "                      f'r1_penalty: {metric_r1_penalty.result():.3f}, '\n",
    "                      f'idt_loss: {metric_idt_loss.result():.3f}, '\n",
    "                      f'pixel_loss: {metric_pixel_loss.result():.3f}, '\n",
    "                      f'g_total: {metric_g_loss.result():.3f}, '\n",
    "                      f'g_gan_loss: {metric_g_gan_loss.result():.3f}, ')\n",
    "\n",
    "                # reset timer\n",
    "                t_start = time.time()\n",
    "\n",
    "            # save every self.save_step\n",
    "            if step % self.save_step == 0:\n",
    "                self.manager.save(checkpoint_number=step)\n",
    "\n",
    "            # save every self.image_summary_step\n",
    "            if step % self.image_summary_step == 0:\n",
    "                # add summary image\n",
    "                real_images_val, labels_val = next(val_ds_iter)\n",
    "\n",
    "                # fake img\n",
    "                fake_images_train, fake_images_val = self.gen_samples(labels, labels_val)\n",
    "\n",
    "                # convert to tensor image\n",
    "                real_images_train = self.convert_per_replica_image(real_images)\n",
    "                real_images_val = self.convert_per_replica_image(real_images_val)\n",
    "                fake_images_train = self.convert_per_replica_image(fake_images_train)\n",
    "                fake_images_val = self.convert_per_replica_image(fake_images_val)\n",
    "\n",
    "                # set batch size\n",
    "                real_images_train = real_images_train[:self.n_samples]\n",
    "                real_images_val = real_images_val[:self.n_samples]\n",
    "                fake_images_train = fake_images_train[:self.n_samples]\n",
    "                fake_images_val = fake_images_val[:self.n_samples]\n",
    "\n",
    "                # merge on batch dimension\n",
    "                t_out = tf.concat([real_images_train, fake_images_train], axis=0)\n",
    "                v_out = tf.concat([real_images_val, fake_images_val], axis=0)\n",
    "\n",
    "                # make single image and add batch dimension for tensorboard\n",
    "                t_out = merge_batch_images(t_out, self.train_res, rows=2, cols=self.n_samples)\n",
    "                v_out = merge_batch_images(v_out, self.train_res, rows=2, cols=self.n_samples)\n",
    "                t_out = np.expand_dims(t_out, axis=0)\n",
    "                v_out = np.expand_dims(v_out, axis=0)\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    tf.summary.image('train_out', t_out, step=step)\n",
    "                    tf.summary.image('val_out', v_out, step=step)\n",
    "\n",
    "            # check exit status\n",
    "            if step >= self.max_steps:\n",
    "                break\n",
    "\n",
    "        # save last checkpoint\n",
    "        step = self.g_optimizer.iterations.numpy()\n",
    "        self.manager.save(checkpoint_number=step)\n",
    "        return\n",
    "\n",
    "    @tf.function\n",
    "    def gen_samples(self, train_labels, val_labels):\n",
    "        fake_images_train = self.g_clone(train_labels, truncation_psi=1.0, training=False)\n",
    "        fake_images_val = self.g_clone(val_labels, truncation_psi=1.0, training=False)\n",
    "        # # run networks\n",
    "        # fake_images_05 = self.g_clone([test_z, test_labels], truncation_psi=0.5, training=False)\n",
    "        # fake_images_07 = self.g_clone([test_z, test_labels], truncation_psi=0.7, training=False)\n",
    "\n",
    "        # # merge on batch dimension: [n_samples, 3, out_res, 2 * out_res]\n",
    "        # final_image = tf.concat([fake_images_05, fake_images_07], axis=2)\n",
    "        return fake_images_train, fake_images_val\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_per_replica_image(nchw_per_replica_images):\n",
    "        as_tensor = nchw_per_replica_images\n",
    "        as_tensor = tf.transpose(as_tensor, perm=[0, 2, 3, 1])\n",
    "        as_tensor = (tf.clip_by_value(as_tensor, -1.0, 1.0) + 1.0) * 127.5\n",
    "        as_tensor = tf.cast(as_tensor, tf.uint8)\n",
    "        return as_tensor\n",
    "\n",
    "\n",
    "def filter_resolutions_featuremaps(resolutions, featuremaps, res):\n",
    "    index = resolutions.index(res)\n",
    "    filtered_resolutions = resolutions[:index + 1]\n",
    "    filtered_featuremaps = featuremaps[:index + 1]\n",
    "    return filtered_resolutions, filtered_featuremaps\n",
    "\n",
    "\n",
    "def main():\n",
    "    # # global program arguments parser\n",
    "    # parser = argparse.ArgumentParser(description='')\n",
    "    # parser.add_argument('--allow_memory_growth', type=str_to_bool, nargs='?', const=True, default=True)\n",
    "    # parser.add_argument('--debug_split_gpu', type=str_to_bool, nargs='?', const=True, default=False)\n",
    "    # parser.add_argument('--use_tf_function', type=str_to_bool, nargs='?', const=True, default=True)\n",
    "    # parser.add_argument('--use_custom_cuda', type=str_to_bool, nargs='?', const=True, default=True)\n",
    "    # parser.add_argument('--model_base_dir', default='./models', type=str)\n",
    "    # parser.add_argument('--tfrecord_dir', default='./tfrecords', type=str)\n",
    "    # parser.add_argument('--train_res', default=512, type=int)\n",
    "    # parser.add_argument('--shuffle_buffer_size', default=1000, type=int)\n",
    "    # parser.add_argument('--batch_size_per_replica', default=4, type=int)\n",
    "    # args = vars(parser.parse_args())\n",
    "\n",
    "    # check tensorflow version\n",
    "    cur_tf_ver = check_tf_version()\n",
    "\n",
    "    # GPU environment settings\n",
    "    # allow_memory_growth()\n",
    "\n",
    "    # network params\n",
    "    # resolutions = [4, 8, 16, 32, 64, 128, 256, 512]\n",
    "    # featuremaps = [512, 512, 512, 512, 256, 128, 64, 32]\n",
    "    resolutions = [4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "    featuremaps = [512, 512, 512, 512, 512, 256, 128, 64, 32]\n",
    "    train_resolutions, train_featuremaps = filter_resolutions_featuremaps(resolutions, featuremaps, 512)\n",
    "    g_params = {\n",
    "        'w_dim': 512,\n",
    "        'labels_dim': 259,\n",
    "        'n_mapping': 8,\n",
    "        'resolutions': train_resolutions,\n",
    "        'featuremaps': train_featuremaps,\n",
    "    }\n",
    "    d_params = {\n",
    "        'labels_dim': 259,\n",
    "        'resolutions': train_resolutions,\n",
    "        'featuremaps': train_featuremaps,\n",
    "    }\n",
    "\n",
    "    # prepare distribute strategy\n",
    "    global_batch_size = 2\n",
    "\n",
    "    # prepare dataset\n",
    "    tfrecord_dir = os.path.join('/content/gdrive/MyDrive/stylegan2_tobigs/data/' 'tfrecords')\n",
    "    train_dataset = get_dataset(tfrecord_dir, 512, batch_size=global_batch_size, is_train=True)\n",
    "    val_dataset = get_dataset(tfrecord_dir, 512, batch_size=global_batch_size, is_train=False)\n",
    "    \n",
    "    # training parameters\n",
    "    training_parameters = {\n",
    "        # global params\n",
    "        'cur_tf_ver': cur_tf_ver,\n",
    "        'use_tf_function': True,\n",
    "        'use_custom_cuda': True,\n",
    "        'model_base_dir': '/content/gdrive/MyDrive/stylegan2_tobigs/models',\n",
    "\n",
    "        # network params\n",
    "        'g_params': g_params,\n",
    "        'd_params': d_params,\n",
    "\n",
    "        # training params\n",
    "        'g_opt': {'learning_rate': 0.002, 'beta1': 0.0, 'beta2': 0.99, 'epsilon': 1e-08, 'reg_interval': 8, 'pixel_lambda': 10.0/(512*512)},\n",
    "        'd_opt': {'learning_rate': 0.002, 'beta1': 0.0, 'beta2': 0.99, 'epsilon': 1e-08, 'reg_interval': 16, 'idt_lambda':1e-3},\n",
    "        'batch_size': global_batch_size,\n",
    "        'n_total_image': 8000000,\n",
    "        'n_samples': 3,\n",
    "        'train_res': 512,\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(training_parameters, name=f'stylegan2-{512}x{512}')\n",
    "    trainer.train(train_dataset, val_dataset)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir '/content/gdrive/MyDrive/stylegan2_tobigs/models/stylegan2-512x512'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
