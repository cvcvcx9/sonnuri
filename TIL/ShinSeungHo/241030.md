# 241030

# 요약: ai 모델 공부

1. BERT (Bidirectional Encoder Representations from Transformers)
   개요
   BERT는 Transformer 아키텍처를 기반으로 한 모델로, 구글에서 개발하였습니다.
   단어의 양방향 문맥을 학습하여 단어가 문장 내에서 갖는 의미를 더욱 정확히 파악할 수 있습니다.
   특징
   양방향 문맥 학습: BERT는 단어를 학습할 때 문장의 앞뒤 정보를 모두 고려하여 학습합니다. 예를 들어, “apple”이라는 단어가 "I ate an apple"과 "Apple Inc."에 사용될 때 각각 다른 의미로 학습됩니다.
   문장과 문장 관계 이해: BERT는 단순히 단어 임베딩뿐 아니라, 문장 간 관계나 전체 문장의 의미까지 이해하도록 학습됩니다.
   Fine-tuning(미세 조정): 다양한 자연어 처리 과제에 맞게 추가 학습을 통해 조정할 수 있습니다. 예를 들어 감정 분석, 질문 답변, 문장 분류 등 다양한 NLP 작업에 쉽게 적용할 수 있습니다.
   장단점
   장점: 문맥 이해도가 높고 다양한 NLP 작업에 활용하기 좋습니다.
   단점: 모델이 크고, 계산량이 많아 속도가 느리며 GPU 리소스가 많이 필요합니다.
   사용 예시
   문장 분류: 텍스트의 긍정/부정 감정 분류
   질문-답변: 주어진 질문에 적절한 답변을 생성

2. FastText
   개요
   FastText는 페이스북 AI 연구소에서 개발한 단어 임베딩 모델로, Word2Vec의 개선된 버전입니다.
   단어를 문자 단위로 쪼개서 학습하는 서브워드 임베딩(subword embedding) 방식을 사용하여 희귀 단어와 새로운 단어의 의미를 학습할 수 있습니다.
   특징
   서브워드(subword) 정보 학습: 단어를 여러 개의 n-gram(예: "apple"을 "app", "ppl", "ple" 등으로 나누어 학습)으로 쪼개서 학습하기 때문에 신조어나 철자가 유사한 단어의 의미도 잘 파악할 수 있습니다.
   단어 임베딩 생성: FastText는 단어 자체에 대한 벡터를 생성하고, 단어 간 유사도를 효율적으로 계산할 수 있습니다.
   낮은 자원 소모: 상대적으로 작은 모델 사이즈와 낮은 연산량으로 빠르게 임베딩을 생성할 수 있습니다.
   장단점
   장점: 빠른 학습 속도와 낮은 리소스 사용량, 신조어나 희귀 단어에도 잘 대응합니다.
   단점: 단어를 독립적으로 학습하기 때문에 문맥 이해가 부족하고, 문장의 의미를 추론하기 어렵습니다.
   사용 예시
   단어 유사도 계산: 단어 간 유사도를 빠르게 계산
   검색 시스템: 유사한 키워드를 통한 빠른 검색 추천
