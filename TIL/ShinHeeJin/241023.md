# 241023
## 요약: 아이디어 기획

## 대상 구체화 아이디어 회의

# 대상 구체화

- 청각장애인 어떤?
1. 농인 미디어 사용률
2. PC 사용
3. 여가 생활 종목
4. 직업
5. 청각장애인으로 범주 확대
    1. 청각장애인이 된 원인 + 연령대
    2. 소통방법
    3. 보청기 단점 - 인공와우
6. 시각적 도움방향
7. 교통수단 음성정보
8. 진동 도움방향
9. 

# 언어확장

번역

---

## 문화생활

- 함께하는 사람이 필요
- 영화관
    - 한국 영화가 오히려 자막이 없어 더 불편함
- OTT이용 어려움

## PC 사용

- 청각장애인의 이용율이 현저히 낮음
- OTT를 통해 정보를 얻고자하는 경우 많음

## 직업

- 경제활동상태가 낮음
- 하고싶은 직업: 사무직 / 현실: 단순노무종사

## 범주확대

- 원인: 유전, 노화, 소음
- 연령대는 상관없음
- 국제 수어가 있음

## 시각적도움

- 영상을 봤을 때 청각장애인들한테 도움이 되는 일(도우미견)
- 소리를 못들어서 도움이 필요한 경우가 많음
- 걱정이 되는건 전 프로젝트랑 비슷할 수도 있다.
- 아이 위기상황을 파악하는 프로젝트를 확대시키는 느낌이다.
    - 세탁기나, 노크 알람을 켰다. 등의 생활음을 학습시켜서 강아지가 알려주는 형태로 도움을 주는 게 있었다. 이걸 할 수 있으면 좋을듯
- 사람들과의 관계를 좀 더 잘 하고싶다.
- 적극적으로 나서는 사람도 있었지만, 정부의 지원이 필요할 수도 있다.

## 운전

- 시각장애인 분들 생각보다 운전 잘함
- 시각적으로 크락션을 보여주는 기능이 이미 존재함.
- 보험사를 보통 전화로 부르는데, 그런 경우 조금 곤란함
- 생각보다 택시를 운영하는 경우가 많음(고요한택시)
- 이미 레드오션

## 진동

- 뉴스쪽으로 많이 나와서 찾아봄
- 애플 음악햅틱 - 진동이랑 화면 반짝거리는걸로 청각장애인 분들도 느낄 수 있도록 함
- 밴드나 오케스트라에 들어가서 연주를 한다는 내용도 있었음
    - 전원이 청각장애인인 아이돌도 있음
    - 모두 일을 할 때 진동으로 도움을 받는다.
- 진동알림 네비도 제품화가 되어있다.


---


# 수어 영상 코드 분석
# preprocessing

## 1. `data_preprocessing.ipynb`

- 수어 데이터셋의 전처리를 담당

### 주요 기능

- 데이터 정리
    - F 데이터만 선별
    - 영상, 형태소Json, 키포인트Json 등에서 F 데이터만 남김
- 영상 프레임 추출
    - 영상에서 특정 시간대의 프레임만 추출
    - 원본 이미지와 512x512 크기로 리사이즈된 이미지 저장
    - 시작(start)과 끝(end) 지점을 형태소 Json에서 추출하여 해당 구간만 처리
- 키포인트 데이터 처리
    - OpenPose에서 추출된 키포인트 데이터 전처리
    - pose, face, left_hand, right_hand 키포인트 통합
    - 0값 처리 (보간법 사용)
    - 좌표값 정규화 (0~1 스케일링)
    - 화자 정보를 one-hot 인코딩으로 변환

## 2. `imgae2video.ipynb`

- 생성된 이미지들을 다시 비디오로 변환
- OpenCV를 사용하여 이미지 시퀀스를 비디오로 변환
- 프레임 레이트 조절 가능 (20fps 사용)
- DIVX 코덱을 사용하여 .avi 형식으로 저장

## 3. `keypoints_preprocessing.ipynb`

- OpenPose에서 추출된 키포인트 데이터를 전처리

> 키포인트 데이터 구조
> 
> - pose_keypoints_2d: 전신 포즈 키포인트
> - face_keypoints_2d: 얼굴 키포인트
> - hand_left_keypoints_2d: 왼손 키포인트
> - hand_right_keypoints_2d: 오른손 키포인트
- 전처리 과정
    
    a) 불필요한 키포인트 제거
    
    - 하반신 키포인트 제거 (인덱스 30-35, 39-44, 57-74)
    - pose 키포인트를 75개에서 45개로 축소
    
    b) 키포인트 통합 및 정리
    
    - pose, face, hand 키포인트를 하나의 1차원 리스트로 연결
    - confidence 값 제외 (x, y 좌표만 사용)
    - 최종적으로 254개의 키포인트 좌표 생성
    
    c) 누락된 키포인트(0값) 처리
    
    ```python
    if i>1 and ZERO:
        zero_tf = keypointss[i-1]==0
        keypointss[i-1][zero_tf] = (keypointss[i-2][zero_tf]+keypointss[i][zero_tf])/2
    ```
    
    - 앞뒤 프레임의 평균값으로 보간
    - 연속된 0값이 없다고 가정
    
    d) 좌표 정규화
    
    ```python
    scale_array[::2] /= 2048# x좌표
    scale_array[1::2] /= 1152# y좌표
    ```
    
    - x, y 좌표를 0~1 사이의 값으로 정규화
    - 2048x1152 해상도 기준
- 저장 형식:
    - 각 프레임마다 한 줄로 저장
    - 형식: "이미지프레임이름_keypoints, [정규화된 키포인트 값 254개]"
    - txt 파일로 저장

## 4. `video_preprocessing.ipynb`

1. F 데이터 선별
    - F가 아닌 영상/형태소/키포인트 데이터 삭제
2. 영상 처리
    - 시작/종료 시간 추출
    - 해당 구간의 프레임만 이미지로 저장
    - 리사이즈 처리
3. 키포인트 데이터 처리
    - 필요없는 키포인트 제거
    - 시작/종료 시간에 맞춰서 Json 파일 정리

## 5. `video_to_img.py`

- 비디오를 프레임 단위 이미지로 변환하는 클래스
- 특징:
    - 10개 단위로 폴더 생성 (0-10, 10-20, ...)
    - 각 비디오의 모든 프레임을 이미지로 저장
    - 옵션으로 프레임 수를 줄일 수 있음 (30프레임당 1장)

## 전체 구조

### 초기 데이터 구조

- 영상 데이터 (mp4 파일)
- 키포인트 Json (OpenPose로 추출된 관절 좌표)
- 형태소 Json (텍스트 데이터)

### 데이터 필터링(`data_preprocessing.ipynb`)

- F 데이터만 선별
- 영상, 형태소Json, 키포인트Json에서 F 데이터만 남김
- 시작/종료 시점 추출

### 영상 프레임 추출(`data_preprocessing.ipynb`)

- 원본 영상에서 필요한 구간의 프레임만 추출
- 원본 이미지와 512x512 크기로 리사이즈된 이미지 저장
- Image/ 폴더와 Resize_Image/ 폴더에 각각 저장

### 키포인트 데이터 처리 (`keypoints_preprocessing.ipynb`)

1. Json 데이터 통합
    - pose, face, hand 키포인트를 하나로 통합
    - 하반신 키포인트 제거 (불필요한 정보)
    - confidence 값 제외
2. 데이터 정제
    - 누락된 키포인트(0값) 보간처리
    - 좌표값 정규화 (0~1 스케일링)
3. 최종 저장
    - txt 파일 형태로 변환
    - 프레임별 키포인트 데이터 저장

### 형태소 분석 (morpheme_parsing.py)

- 형태소 Json 파일 파싱
- 텍스트 데이터 추출 및 처리

### 최종 결과물

- 전처리된 이미지 프레임
- 정규화된 키포인트 데이터
- 처리된 텍스트 데이터

## 실제 사용되는 핵심 파일들:

- data_preprocessing.ipynb
- keypoints_preprocessing.ipynb
- morpheme_parsing.py

## 추가/보조 파일들:

- morpheme_parsing_test.ipynb (형태소 파싱 테스트 버전)
- video_to_img.py (비디오->이미지 변환 별도 버전)
- Untitled.ipynb (키포인트 시각화 테스트)
- image2video.ipynb (이미지->비디오 변환, 결과 확인용)
- video_preprocessing.ipynb (영상 전처리 다른 버전)

---

# Text2keypoint

## `data/data.py`

1. 데이터 로드 및 전처리
    - 3가지 입력 파일 처리:
        - .files: 각 시퀀스의 이름
        - .gloss: 소스 문장 (텍스트)
        - .skels: 스켈레톤 데이터 (키포인트)
2. 데이터 변환 및 처리
    
    def make_y():
    
    - 키포인트 데이터를 Tacotron 학습 모델에 맞게 변환
    - 254개의 포즈 키포인트를 처리
    - 결과: (문장 수, 최대 프레임 수, trg_size) 형태
    
    def make_output_data():
    
    - 파일명, 글로스(텍스트), 스켈레톤 데이터 전처리
    - SL_ 와 _F 사이의 텍스트 추출
    - 공백을 언더스코어로 변환
3. 임베딩 처리
    - Tokenizer를 사용하여 텍스트를 시퀀스로 변환
    - 패딩 처리로 문장 길이 통일
    - 단어 임베딩을 위한 vocab_size 계산
4. 모델 입력값 생성
    - 디코더 입력 배열 생성
    - mel spectrogram 데이터 배열 생성
    - 프레임 수에 따른 패딩 처리
5. return
    
    학습 모드:
    
    - X_: 텍스트 시퀀스
    - y: 키포인트 데이터
    - decoder_input_array: 디코더 입력
    - mel_spectro_data_array: mel spectrogram 데이터
    - max_X: 문장의 최대 어휘 수
    - vocab_size_source: 단어 임베딩 크기
    
    테스트 모드:
    
    - X_, y, decoder_input_array, mel_spectro_data_array만 반환

## `modeling/helpers.py` - 유틸리티 함수

- load_config(): YAML 설정 파일을 로드
- make_dir(): 새 디렉토리 생성

## `modeling/key2video.py`

- 키포인트를 스틱피겨 이미지와 영상으로 변환

**주요 기능:**

- create_stick(): JSON 키포인트 파일로 스틱피겨 이미지 생성
    - pose, face, hand 포인트 페어 정의
    - 각 키포인트를 연결하여 스틱피겨 생성
- create_video(): 이미지 시퀀스를 비디오로 변환
    - fps=30으로 설정
    - DIVX 코덱 사용
- create_img_video(): 메인 함수
    - JSON 파일 로드
    - 스틱피겨 이미지 생성
    - 비디오 생성

## `modeling/model.py`

- Tacotron 기반 모델 구축
- build_model(): 모델 구성
    - encoder: 텍스트 인코딩
    - decoder_prenet: 디코더 전처리
    - attention: 어텐션 메커니즘
    - decoder: 최종 출력 생성

## `modeling/prediction.py`

- 모델 예측 및 결과 생성
- make_predict():
    - 모델 예측 수행
    - DTW 스코어 계산
    - JSON 키포인트 파일 생성
    - 스틱피겨 이미지/비디오 생성

## `modeling/tacotron.py`

- Tacotron 아키텍처 구현
- 주요 컴포넌트:
    - Encoder: 텍스트를 잠재 표현으로 변환
    - Decoder_prenet: 디코더 입력 전처리
    - Attention: 인코더-디코더 간 어텐션
    - Decoder: 최종 출력 생성

각 컴포넌트는 CBHG(1D convolution, Highway net, bidirectional GRU) 구조 사용

## `Base.yaml`

- 설정 파일
- 주요 설정:
    - mode: "Train" 또는 "Test"
    - 데이터 경로 및 모델 경로 설정
    - 데이터 관련 설정 (src, trg, max_sent_length 등)
    - 훈련 관련 설정 (batch_size, epochs 등)
    - 모델 관련 설정 (latent_dim, N_MEL 등)

## `main.ipynb`

- Google Colab에서 GPU 사용을 위한 설정
- 드라이브 마운트
- 깃허브 클론
- Base.yaml 모드 설정
- **main**.py 실행

## `train.py`

- MyCallback 클래스: 학습 중 모델 저장 및 예측 생성
- 데이터 로드 및 전처리
- 모델 구축
- 콜백 설정 (모델 체크포인트, LR 스케줄러 등)
- 모델 학습 및 저장

## `test.py`

- 테스트 데이터 로드
- 전처리된 데이터 로드
- 모델 로드 (best 또는 recent)
- 예측 수행 및 결과 생성

---

# stick_video

## `keypoints_to_stick_image.py`

- OpenPose 키포인트를 스틱피겨 이미지로 변환
- **주요 구성요소:**
a) 키포인트 페어 정의
    - pose_point_pair_ver1, ver2: 전신 포즈 연결점
    - face_point_pair: 얼굴 연결점
    - hand_point_pair: 손 연결점 ([0,1], [1,2], ...)
    
    b) draw_keypoints() 함수
    
    - 이미지에 키포인트와 선 그리기
    - 키포인트: 노란색 원
    - 연결선: 빨간색 선
    
    c) create_stick() 함수
    
    - 1500x1500 흰색 이미지 생성
    - 4가지 파트 처리:
        - pose_keypoints_2d (25점)
        - face_keypoints_2d (68점)
        - hand_left_keypoints_2d (21점)
        - hand_right_keypoints_2d (21점)

## `stick_image_to_video.py`

- 스틱피겨 이미지 시퀀스를 비디오로 변환
- 이미지 파일 정렬하여 로드
- fps=30으로 설정
- DIVX 코덱 사용
- 이미지 시퀀스를 비디오로 저장

---

# Everybodydance

## Everybodydance/data_loader

BaseDataLoader (기본 클래스)
↓
CustomDatasetDataLoader (구현 클래스)
↓
AlignedDataset (실제 데이터셋 처리)

## `aligned_dataset.py`

- 레이블 맵과 실제 이미지 쌍을 처리
- 데이터셋 구조:
    - train_label: 레이블 맵
    - train_img: 실제 이미지
    - train_facetexts128: 얼굴 바운딩 박스 (옵션)
- 변환 기능: 크기 조정, 자르기, 뒤집기 등

## `base_dataset.py`

- 데이터셋의 기본 기능 정의
- 이미지 처리 유틸리티 함수:
    - get_params(): 크기 조정 파라미터
    - get_transform(): 이미지 변환 파이프라인
    - normalize(): 정규화

## `base_data_loader.py`:

- 데이터 로더의 기본 인터페이스 정의
- 최소한의 구조만 제공

## `custom_dataset_data_loader.py`

- AlignedDataset 생성
- PyTorch DataLoader 설정
- 배치 크기, 셔플, 쓰레드 수 등 설정

## `data_loader.py`:

- 데이터 로더 생성 함수들:
  * CreateDataLoader
  * CreateDataLoaderU
  * CreateTransferDataLoader

## `image_folder.py`

ImageFolder (기본 이미지 로딩)
↓
AlignedDataset (레이블-이미지 쌍 처리)
↓
CustomDatasetDataLoader (데이터 로더 구성)

---

# Everybodydance/data_prep

## `graph_avesmooth.py`: 키포인트 데이터 평균화/평활화 처리

- 윈도우 사이즈만큼의 프레임에서 키포인트 평균 계산
- pose, face, hand 각각의 키포인트 처리
- 좌표 스케일링 및 정규화
- 스틱피겨 이미지 생성
- facetexts(얼굴 바운딩 박스) 생성

## `graph_facebox.py`: 얼굴 바운딩 박스 생성

- 키포인트에서 얼굴 영역 계산
- 128x128 크기의 얼굴 바운딩 박스 생성
- 바운딩 박스 좌표 조정 및 저장

## `graph_posenorm.py`: 포즈 정규화

- 소스와 타겟 비디오 간의 스케일과 변환 계산
- 포즈 키포인트 변환/정규화
- 변환된 포즈로 스틱피겨 생성
- 결과 저장 (레이블, 이미지, 얼굴 텍스트)

## `graph_train.py`: 학습 데이터 생성

- 키포인트 데이터 로드
- 스케일링 및 리사이징
- 스틱피겨 렌더링
- 학습용 데이터셋 생성:
  * train_label: 스틱피겨
  * train_img: 원본 이미지
  * train_facetexts128: 얼굴 바운딩 박스
  * train_handtexts90: 손 바운딩 박스

## `get_facetexts.py`: 얼굴 영역 추출

- 포즈 키포인트에서 얼굴 위치 계산
- 바운딩 박스 생성 (128x128)
- 얼굴 영역 이미지 저장

### 전체 데이터 전처리 파이프라인:

1. 데이터 정규화
    
    - 포즈 정규화 (graph_posenorm.py)
    - 키포인트 평활화 (graph_avesmooth.py)
    
2. 영역 추출
    
    - 얼굴 바운딩 박스 (graph_facebox.py)
    - 손 바운딩 박스 (train에서 처리)
    
3. 학습 데이터 생성
    
    - 스틱피겨 렌더링
    - 이미지 리사이징
    - 바운딩 박스 정보 저장
    

## `pose_object.py`: 포즈 데이터를 위한 클래스

class Pose:
- 키포인트 데이터를 객체화
- 구성 요소:
  * keyname: 키포인트 파일명
  * posepts: 포즈 키포인트
  * facepts: 얼굴 키포인트
  * rhandpts/lhandpts: 오른손/왼손 키포인트
- 손 데이터 업데이트 메서드 제공

## `renderopenpose.py`: OpenPose 출력 렌더링을 위한 핵심 기능

a) 키포인트 파일 읽기
- readkeypointsfile(): yml/json 파일 지원
- map_25_to_23(): 25점 포즈를 23점으로 변환

b) 스케일링/리사이징
- scale_resize(): 이미지 크기 조정
- fix_scale_coords(): 좌표 스케일링

c) 렌더링 함수들
- renderpose(): COCO/23점/25점 포맷 지원
- renderface(): 얼굴 키포인트 렌더링
- renderhand(): 손 키포인트 렌더링

d) 분석 함수들
- aveface(): 얼굴 중심점 계산
- get_pose_stats(): 포즈 통계 계산
- getmedians_adapt(): 적응형 중앙값 계산

### 데이터 처리 흐름

→ Pose 객체 생성 (pose_object.py)
→ 키포인트 렌더링 (renderopenpose.py)
→ 이미지/영상 생성 (graph_*.py 파일들)